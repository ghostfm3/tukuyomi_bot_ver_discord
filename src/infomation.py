import requests
import configparser
import urllib.request
import ginza
import spacy
from bs4 import BeautifulSoup
from xml.etree import ElementTree

config_ini = configparser.ConfigParser()
config_ini.read('config.ini', encoding='utf-8')

class TakeInformation:
    def __init__(self):
        '''
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿å®šç¾©
        '''
        self.quake_url = config_ini['URLS']['quake']
        self.weather_url =  config_ini['URLS']['weather'] 
        self.attention_url = config_ini['URLS']['attention'] 
        self.client_id = config_ini['TOKENS']['TWID']
        self.tokens = config_ini['TOKENS']['TTOKEN']
        self.client_secret = config_ini['TOKENS']['TSC']
        
        # spreadsheet
        self.spreadsheet = config_ini['URLS']['spreadsheets']
        self.drive = config_ini['URLS']['googledrive']
        self.sheetinfo = config_ini['URLS']['spreadsheet_url']
    
    def take_earthquake_info(self):
        '''
        æœ€æ–°ã®åœ°éœ‡æƒ…å ±ã‚’1ä»¶å–å¾—ã™ã‚‹
        '''
        e_result_list = []
        m_scale_dict = {
            10:'éœ‡åº¦1',
            20:'éœ‡åº¦2',
            30:'éœ‡åº¦3',
            40:'éœ‡åº¦4',
            45:'éœ‡åº¦5å¼±',
            50:'éœ‡åº¦5å¼·',
            55:'éœ‡åº¦6å¼±',
            60:'éœ‡åº¦6å¼·',
            70:'éœ‡åº¦7'
        }
        quake_json = requests.get(self.quake_url).json()
        quake_info = quake_json[0]["earthquake"]
        quake_info_plus = quake_json[0]["earthquake"]["hypocenter"]
        
        time = quake_info["time"]
        m_scale = quake_info["maxScale"]
        point = quake_info_plus["name"]
        depth = quake_info_plus["depth"]
        scale = quake_info_plus["magnitude"]
        point1 = quake_info_plus["latitude"]
        point2 = quake_info_plus["longitude"]
        
        m_scale_p = m_scale_dict[m_scale]
        e_result_list = [time, point, depth, scale, point1, point2, m_scale_p]
        
        return e_result_list
    
    def take_weather_information(self):
        '''
        æ±äº¬ãƒ»å¤§é˜ªã®å½“æ—¥ã®å¤©æ°—äºˆå ±ã‚’å–å¾—ã—ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿”ã™
        '''
        t_w_list = self.weather_json("130010")
        o_w_list = self.weather_json("270000")
        
        return t_w_list, o_w_list
        
    def weather_json(self, location_code):
        '''
    ã€€ã€€URL+åœ°åŸŸã‚³ãƒ¼ãƒ‰ã‚’å…ƒã«å¤©æ°—æƒ…å ±ã‚’jsonã‹ã‚‰å–å¾—ã™ã‚‹
        '''
        w_result_list = []
        url = self.weather_url + location_code
        weather_json = requests.get(url).json() 
        date = weather_json["forecasts"][0]["date"]
        locate = weather_json["location"]["prefecture"]
        weather = weather_json["forecasts"][0]["detail"]["weather"]
        telop = weather_json["forecasts"][0]['telop']
        
        if "æ™´ã‚Œ" in telop:
            telop = "æ™´"
        elif "æ›‡ã‚Š" in telop:
            telop = "æ›‡"
        elif "ä¸€æ™‚" in telop:
            telop = "ä¸€"
        
        wakati_list, doc = self.ja_ginza_token(telop)
        emoji_str = self.convert_weather_string(wakati_list)
        
        w_result_list = [date, locate, weather, emoji_str]
        
        return w_result_list
    
    def ja_ginza_token(self, word:str) -> list:
        '''
        GiNZAã§ã®åˆ†ã‹ã¡æ›¸ã
        '''
        nlp = spacy.load('ja_ginza')
        ginza.set_split_mode(nlp, 'C')
        doc = nlp(word)
        tokens = [token.text for token in doc]
        return tokens, doc
    
    def convert_weather_string(self, weather_list:list) -> str:
        '''
        å¤©æ°—äºˆå ±æ–‡ã‚’çµµæ–‡å­—ã«å¤‰æ›ã™ã‚‹
        '''
        convert_list = []
        weather_emojis = {
            "æ™´": "â˜€ï¸",
            "æ›‡": "â˜ï¸",
            "é›¨": "ğŸŒ§ï¸",
            "é›ª": "â„ï¸",
            "ã®ã¡":"->",
            "æ™‚ã€…":"/",
            "ä¸€":"/"
        }

        for w_str in weather_list:
            conv = weather_emojis[w_str]
            convert_list.append(conv)

        result = ''.join([w for w in convert_list])
        return result
    
    def get_access_token(self):
        '''
        Twitchã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯å–å¾—
        '''
        token_url = "https://id.twitch.tv/oauth2/token"
        payload = {
            "client_id": self.client_id,
            "client_secret": self.client_secret,
            "grant_type": "client_credentials"
        }
        response = requests.post(token_url, data=payload)

        if response.status_code == 200:
            data = response.json()
            access_token = data["access_token"]
            return access_token
        else:
            return None
    
    def check_stream_status(self, streamer_name):
        '''
        Twitchã®é…ä¿¡æƒ…å ±ã‚’å–å¾—ã—ã¦è¿”ã™
        '''
        access_token = self.get_access_token()
        
        headers = {
            'Client-ID': self.client_id,
            'Authorization': f'Bearer {access_token}',
        }

        url = f'https://api.twitch.tv/helix/streams?user_login={streamer_name}'
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            data = response.json()
            if data['data']:
                data_list = []
                user_id = data['data'][0]['user_login']
                name = data['data'][0]['user_name']
                title = data['data'][0]['title']
                data_list.append(user_id)
                data_list.append(name)
                data_list.append(title)
                return True, data_list # é…ä¿¡ä¸­
            else:
                data_list = None
                return False, data_list  # é…ä¿¡ã—ã¦ã„ãªã„
        else:
            print(response.text)
            data_list = None
            
        return None, data_list# ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
    
    def take_base_attention_info(self):
        '''
        æ³¨æ„å ±æƒ…å ±æ›´æ–°å–å¾—
        '''
        soup = self.take_url_info(self.attention_url)
        res_list = []
        entries = soup.find_all("entry")
        latest_entry = None
        latest_updated = None

        for entry in entries:
            title = entry.find("title").text
            if title == "æ°—è±¡è­¦å ±ãƒ»æ³¨æ„å ±ï¼ˆï¼¨ï¼’ï¼—ï¼‰":
                updated = entry.find("updated").text
                if latest_entry is None or updated > latest_updated:
                    latest_entry = entry
                    latest_updated = updated

        if latest_entry is not None:
            id_updated = latest_entry.find("updated").text
            id_url = latest_entry.find("id").text
        else:
            res_list = None
        
        res_list = [id_updated, id_url]
        
        return res_list
    
    def url_to_take_attention(self, url):
        '''
        è©³ç´°æƒ…å ±å–å¾—
        '''
        attention_list = []
        code_list = []
        soup = self.take_url_info(url)
        
        information = soup.find("Information", {"type": "æ°—è±¡è­¦å ±ãƒ»æ³¨æ„å ±ï¼ˆåºœçœŒäºˆå ±åŒºç­‰ï¼‰"})
        if information is not None:
            information2 = soup.find("Headline")
            head = information2.find_all("Text")
            text = head[0].text
            items = information.find_all("Item")
            for item in items:
                names = item.find_all("Name")
                codes = item.find_all("Code")
                for name in names:
                    attention_list.append(name.text)
                for code in codes:
                    code_list.append(code.text)
        else:
            attention_list = None
            code_list = None
            text = None
        
        return attention_list, code_list, text
        
    def take_url_info(self, url):
        '''
        å‡¦ç†å…±é€šåŒ–
        '''
        res = urllib.request.urlopen(url).read()
        soup = BeautifulSoup(res, "xml")
        
        return soup